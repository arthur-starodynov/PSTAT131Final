---
title: "Laptop Price Amongst Varying Suppliers"
author: "Arthur Starodynov (7751472)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: readable
    toc: yes
    code_folding: hide
    
---
## Intro
### Intro

The purpose of this report is to figure out which brand of laptops provides the best laptop for the cheapest price. 

### Why might our predictive model be helpful? 

The reason behind making our model useful and helpful is to guide customers to make an educated decision on the best laptop for their specific needs. Some customers might be intrigued and unsatisfied when a predictive model can show that some companies are not showing the fine print or real performance of the laptop.




```{r, echo=FALSE, include = FALSE}
#Load the Necessary Packages
set.seed(2022)
library(googledrive,quietly=TRUE)
library(MASS,quietly=TRUE)
library(glmnet,quietly=TRUE)
library(pROC,quietly=TRUE)
library(rpart,quietly=TRUE)
library(randomForest, quietly=TRUE)
library(ranger, quietly=TRUE)
library(caret, quietly=TRUE)
library(factoextra, quietly=TRUE) # A special printing library for clustering analyses 
library(devtools, quietly=TRUE)
#library(ggbiplot, quietly=TRUE)
library(ggplot2, quietly=TRUE)
library(zoo, quietly = TRUE)
library(tm)
library(cluster)
library(SnowballC)
library(xgboost)
```

```{r, collapse = TRUE}
#Read in the data
laptop.data <- read.csv('C:/Users/arthu/Dropbox/My PC (DESKTOP-9BV8I37)/Documents/PSTAT131Final/Cleaned_Laptop_data.csv.csv')
set.seed(46)
```

## Data
### Introduction into the data set
As we all may know there are various laptop brands out there, with some of the most popular being Dell, Mac, and Lenovo. However, some of these popular laptop brands can be cheaping out customers based on their reputation that they have already built up. For example due to Apple’s brand recognition and amazing customer support, the best performance laptop is mainly focused on performance features such as RAM, the Graphics Card, a Solid State Drive, and other factors. In addition, a factor that may persuade customers to buy a specific laptop would be based on online reviews and generic ratings that professionals may include. Our goal through using this data set found on Kaggle, is to be able to figure out which laptop brand really bring the most high performing laptop with the best price.  The data set below has 896 observations and 23 variables. 
Just by looking at the column names all of them are self explanatory where they state what is being described in each column. 


Here we looked at what the values of our columns are within our data set. 

```{r, echo = TRUE} 
laptop.data$ram_gb<-gsub(" GB GB","",as.character(laptop.data$ram_gb))
laptop.data$ssd<-gsub(" GB","",as.character(laptop.data$ssd))
laptop.data$hdd<-gsub(" GB","",as.character(laptop.data$hdd))
laptop.data$os_bit<-gsub("-bit","",as.character(laptop.data$os_bit))

print("Table view of all categorical variables")

```

```{r, echo=FALSE, results= "Hide"}
names=colnames(laptop.data)

for (x in 1:17) {
  print(paste("Table view for column: ", names[x]))
  print(table(laptop.data[,x]))
}
laptop.data[ laptop.data == "Missing" ] <- NA
colSums(is.na(laptop.data))

laptop.data[ laptop.data == "Missing" ] <- NA

colSums(is.na(laptop.data))

```

## Visualization
### First Visualization 

We first look at our target variable, Latest Price, convert it into USD $ to make it easier to visualize and notice that the variable is right-skewed. Hence we apply a log transform and make a new target variable which is the log_transform of latest price. 

```{r} 
laptop.data$latest_price <- laptop.data$latest_price*0.013
laptop.data$old_price <- laptop.data$old_price*0.013
d <- density(laptop.data$latest_price)
plot(d, main="Kernel Density of Latest Price")
polygon(d, col="red", border="blue")

```


```{r, echo = TRUE, results = 'HIDE'} 
laptop.data$log_latest_price <- log(laptop.data$latest_price)
d <- density(laptop.data$log_latest_price)
plot(d, main="Kernel Density of Log Latest Price")
polygon(d, col="red", border="blue")
```


### Cleaning of the data 


We noticed that there were a few variables within our data set that had missing observations, so we needed to create a new column that would indicate if display size was given, and gave it either a “Yes or a “No value. In addition we wanted to use all the available data we had so any missing records were replaced with an average of the column, which were eventually grouped to make some levels, furthering the easiness of modeling later. In the clean data, we wanted to drop many of the columns that included N/A which were seen as processor_gnrtn, discount, model, old_price. Also we wanted to only maintain the same currency so we removed the discount, and old_price columns to not get clustered. With all the cleaning we made a new dataframe. 


```{r, collapse= TRUE} 
laptop.data["display_size_given"] <- laptop.data$display_size
laptop.data$display_size_given[!is.na(laptop.data$display_size_given)] <- "Yes"
laptop.data$display_size_given[is.na(laptop.data$display_size_given)] <- "No"
laptop.data$display_size <- as.numeric(laptop.data$display_size)
laptop.data$display_size <- round(na.aggregate(laptop.data$display_size),2)

laptop.data$brand <- tolower(laptop.data$brand)

laptop.data$brand <- ifelse(laptop.data$brand %in% c("acer",
                                                   "apple", "asus", "dell", 
                                                   "hp","lennovo", "msi"),
                            laptop.data$brand,"other")


laptop.data$processor_brand <- tolower(laptop.data$processor_brand)

laptop.data$processor_brand <- ifelse(laptop.data$processor_brand %in% c("amd",
                                                   "intel"),
                            laptop.data$processor_brand,"other")

laptop.data$ram_gb_cat <- ifelse(as.numeric(laptop.data$ram_gb)<=8,
                            "less_than_8","greater_than_8")

laptop.data$ssd<-as.numeric(laptop.data$ssd)
laptop.data$ssd_cat <- ifelse(laptop.data$ssd < 1024.0,
                            "less_than_1gb","greater_than_1gb")

laptop.data$hdd<-as.numeric(laptop.data$hdd)
laptop.data$hdd_cat <- ifelse(laptop.data$hdd < 1024.0,
                            "low_hdd","high_hdd")

laptop.data$processor_name <- tolower(laptop.data$processor_name)

laptop.data$processor_name <- ifelse(laptop.data$processor_name %in% c("celeron dual",
                                                   "core i3", "core i5", "core i7", 
                                                   "m1","pentium quad", 
                                                   "ryzen 3","ryzen 5",
                                                   "ryzen 7","ryzen 9"),
                            laptop.data$processor_name,"other")
laptop.data.clean <-laptop.data[c(1,3,4,7,10,11,12,13,14,15,16,17,21,22,23,24,25,26,27,28)]

```

## Modeling
### Clustering of Data 

Instead of using k folding technique we thought it would be more beneficial to be able to analyze using clustering analysis on the data set. Looking at the data set, we know that RAM, SSD, and the graphic card memory are performance attributes that customers will consider when they are in the market of buying a laptop. Hence, we decided to cluster upon these three columns. 

```{r, warning=FALSE} 
tmp.laptop.data.clean <- laptop.data[,c(6,8,12)]
tmp.laptop.data.clean$ram_gb<- as.numeric(tmp.laptop.data.clean$ram_gb)
tmp.laptop.data.clean$ssd<- as.numeric(tmp.laptop.data.clean$ssd)
tmp.laptop.data.clean$graphic_card_gb<- as.numeric(
  tmp.laptop.data.clean$graphic_card_gb)

tmp.laptop.data.clean <- scale(tmp.laptop.data.clean)
rownames(tmp.laptop.data.clean) <- paste(laptop.data.clean$brand,",",
                              c(1:length(laptop.data.clean$brand)),sep="")

k1 <- kmeans(tmp.laptop.data.clean, centers = 2, nstart = 25)
fviz_cluster(k1, data = tmp.laptop.data.clean, labelsize=6)
k2 <- kmeans(tmp.laptop.data.clean, centers = 4, nstart = 25)
fviz_cluster(k2, data = tmp.laptop.data.clean, labelsize=6)

fviz_nbclust(tmp.laptop.data.clean, kmeans, method = "wss")

gap_stat <- clusGap(tmp.laptop.data.clean, FUN = kmeans, nstart = 25,
                    K.max = 20, B = 50)
fviz_gap_stat(gap_stat)

```
From the above results we notice that we were successful in clustering our laptop variables into separable clusters. Using a classic clustering method to find the optimal amount of them known as the elbow method, we figured out that the optimal amount of clusters is 9, so using 9 centers and the k means method we visualized these variables. 



```{r}
k3 <- kmeans(tmp.laptop.data.clean, centers = 9, nstart = 25)
fviz_cluster(k3, data = tmp.laptop.data.clean, labelsize=6)
```
Below we printed the number of observations in each cluster which we will use to further visualize laptop price. 

```{r} 
laptop.data.clean['cluster'] <- as.factor( k3$cluster)
table(laptop.data.clean$cluster)
```

### Training the Data 

Here we wanted to to explore the data further so we implemented the technique to be able to fix the data into a training and testing data set. 


```{r, collapse=TRUE} 
train.indices <- sample(nrow(laptop.data.clean), floor(nrow(laptop.data.clean)/1.5), replace = FALSE)
validation.indices <- seq(nrow(laptop.data.clean))[-train.indices]
pred.laptop.train <- laptop.data.clean[train.indices,]
pred.laptop.train <- pred.laptop.train[,c(16,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,18,19,20,21)]
pred.laptop.validation <- laptop.data.clean[validation.indices,]
pred.laptop.validation <- pred.laptop.validation[,c(16,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,18,19,20,21)]
```

## Model Peformance
### Analysis
Below we create our "recipe" or formula which we will use to fit our training data set. We wanted to use lasso regression to be able make the model have fewer features. We also applied a stepwise reggression approach onto our model and compared the two.  


```{r}
glmnet.formula <- as.formula(log_latest_price ~ .)
glmnet.design.matrix <- model.matrix(glmnet.formula, data = pred.laptop.train)
dim(glmnet.design.matrix)

glmnet.cv.laptop.out <- cv.glmnet(glmnet.design.matrix, 
                     y = pred.laptop.train$log_latest_price,
                     family = c("gaussian"), 
                     type.measure="mse", # the model selection criteria
                     alpha = 1) # The Lasso regression


plot(glmnet.cv.laptop.out)

saved.coef <- coef(glmnet.cv.laptop.out, s=c("lambda.1se"))

dim(saved.coef)
chosen.vars <- data.frame(name = saved.coef@Dimnames[[1]][saved.coef@i + 1], 
                          coefficient = saved.coef@x)


print(paste("The lasso regression chose", dim(chosen.vars)[1]-1,
            "variables and 1 intercept")) 


print(saved.coef)

glmnet.formula2 <- as.formula(log_latest_price ~ .)
glmnet.design.matrix.validation <- model.matrix(glmnet.formula2,
                                                data = pred.laptop.validation)

validation.preds.regularizedreg <- predict(glmnet.cv.laptop.out,
                                  newx = glmnet.design.matrix.validation,
                                  type = c("response"))
```

We can clearly see that the lasso model was fairly accurate, the stepwise regression model was still superior.

### Decision Tree 

We wanted further visualization of the laptop price variable, hence we made a regression tree to visualize this. 

```{R}
tree.out.1 <-rpart(log_latest_price ~ ., data = pred.laptop.train,
                   parms  = list(split="information"),
                   control = rpart.control(minsplit=20))
```

```{r, include = FALSE}
summary(tree.out.1)
```

```{r}
#Create a plot of the classification tree.
#Code to plot the tree.
plot(tree.out.1, uniform=TRUE, branch=0.6, margin=0.05)
text(tree.out.1, all=TRUE, use.n=TRUE)
title("Laptop Price Regression Tree")

```

Looking at the results we were able to see that we got an R-squared value of around .68 for this model 

### Random Forest Model

We then used the random forest model to see if this would be a better model, and in fact was. 

```{r} 
laptop.train.rf <- randomForest(log_latest_price ~ .,  
                       data = pred.laptop.train, 
                       importance=TRUE)  

print(laptop.train.rf$importance)
varImpPlot(laptop.train.rf)

optimum <- which.max(laptop.train.rf$importance[,"%IncMSE"])
opt.var <- laptop.train.rf$importance[optimum,0,drop=FALSE]

print("The most predictive variable with regard to price is:")
print(opt.var)

val.preds.rf <- predict(laptop.train.rf, # The forest
                 newdata = pred.laptop.validation, # The values of x to do prediction at
                 type = c("response")
                 )

# Code to plot the predictions against the actual values

plot(val.preds.rf, pred.laptop.validation$log_latest_price,
     main = "Plot of Predictions vs. Actual for Laptop Price",           
     xlab = "Log Predicted price of laptop", 
     ylab = "Log Actual price of laptop")

```

Looking at the summary of the model we were able to get an R-squared value of .78

### Best Fitting Model

Through the models created it was clear that the Random Forest provided us with the best analysis of the laptop price data. This was seen as it had the highest R-squared value of around .78. It was seen that processor name variables followed by cluster and ssd_cat were the most predictive towards laptop price.

## Conclusion
### Conclusion
This data set was interesting to visualize and to implement a clustering methodolgy seemed to be more practice than using cross folding. Seeing our best fitting moel was the random forest model, was great, however, was still not as accurate we would have liked it to be. Being able to model this data set and see some correlation of variables to predict laptop price could be used for consumers to have a second thought of which laptop brand they should consider purchasing. In the future if we wanted to expand on the data set, we could instead try using cross folding to mode the data, as well as using an XGboost model to look further at the prediction of laptop price. 
